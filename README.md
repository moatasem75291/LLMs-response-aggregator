# LLM Response Aggregator

A tool to automatically query multiple LLM providers (ChatGPT, Mistral, Grok), evaluate their responses, and determine the best answer.

## Overview

The LLM Response Aggregator is a Python application that helps you:

- Send the same query to multiple LLMs simultaneously
- Extract their responses using browser automation
- Compare model outputs side by side
- Evaluate and rank responses based on relevance, content similarity, and length
- Return the best response and save all results for comparison

## Features

- **Multi-LLM Support**: Currently supports Mistral, and Grok
- **Browser Automation**: Handles web interactions automatically
- **Response Evaluation**: Ranks answers based on query relevance, cross-checking with other LLMs, and optimal length
- **Headless Mode**: Can run without visible browsers for background operation
- **Results Storage**: Saves all responses and their scores in JSON format

## Installation

### Prerequisites

- Python 3.10+
- Chrome browser installed
- ChromeDriver compatible with your Chrome version

### Setup

1. Clone this repository:

```bash
git clone https://github.com/moatasem75291/LLMs-response-aggregator.git
cd llm-response-aggregator
```

2. Build the Docker image:

```bash
docker build -t llm_aggregator .
```

3. Run the Docker container:

```bash
docker run -d -p 8000:8000 --name llm_aggregator_container llm_aggregator
```

4. (Optional) If you want to customize environment variables like email and password, you can modify .env during the build:

```Dockerfile
RUN echo -e "DEEPSEEK_EMAIL=\"PUT-YOUR-EMAIL-HERE\"\nDEEPSEEK_PASSWORD=\"PUT-YOUR-PASS-HERE\"" > .env
```

### Usage

#### Basic Usage

Run the tool with a query:

```bash
curl -X POST -H "Content-Type: application/json" \
-d '{"query": "Explain quantum computing", "llms": ["chatgpt", "mistral"], "headless": false}' \
http://localhost:8000/aggregate
```

- Available Parameters:

  - **query** (str): The question you want to ask. _(Required)_
  - **llms** (list): List of LLMs to query (e.g., `chatgpt`, `deepseek`, `grok`, `mistral`). _(Default: All LLMs)_
  - **headless** (bool): Run browsers in headless mode for faster execution. _(Default: True)_

#### Request Example

```json
{
  "query": "Hi how are you? Can you help me write a function using Python to add two numbers?",
  "llms": ["chatgpt", "mistral"],
  "headless": false
}
```

#### Response Example

```json
{
  "status_code": 200,
  "detail": {
    "original_query": "Hi how are you? Can you help me write a function using Python to add two numbers?",
    "best_response": {
      "source": "chatgpt",
      "content": "Hi! I'm doing great, thanks for asking! ðŸ˜Š I'd be happy to help you write a function in Python to add two numbers.\nHere's a simple function for that:\npython\nCopy\ndef add_numbers(a, b):\n    return a + b\nYou can call this function by passing two numbers as arguments:\npython\nCopy\nresult = add_numbers(3, 5)\nprint(result)  # This will print 8\nLet me know if you'd like any modifications or further explanation!",
      "score": 0.8276620791112479,
      "timestamp": "2025-03-09T15:23:39.027032"
    },
    "all_responses": [
      {
        "source": "chatgpt",
        "content": "Hi! I'm doing great, thanks for asking! ðŸ˜Š I'd be happy to help you write a function in Python to add two numbers.\nHere's a simple function for that:\npython\nCopy\ndef add_numbers(a, b):\n    return a + b\nYou can call this function by passing two numbers as arguments:\npython\nCopy\nresult = add_numbers(3, 5)\nprint(result)  # This will print 8\nLet me know if you'd like any modifications or further explanation!",
        "score": 0.8276620791112479,
        "timestamp": "2025-03-09T15:23:39.027032"
      },
      {
        "source": "mistral",
        "content": "Hello! I'm here to help. Sure, I can help you write a Python function to add two numbers. Here's a simple example:\nCopy\ndef add_numbers(a, b):\n    return a + b\n\n# Example usage:\nresult = add_numbers(3, 5)\nprint(result)  # Output: 8\nIn this function, a and b are the parameters that represent the two numbers you want to add. The function returns the sum of a and b. You can call this function with any two numbers to get their sum.",
        "score": 0.7776620791112479,
        "timestamp": "2025-03-09T15:23:34.012462"
      }
    ],
    "filename": "results/llm_responses_20250309_152339.json"
  },
  "headers": null
}
```

### Project Structure:

```css
llm_aggregator/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py                   # Entry point
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm_configs.py        # Configuration for each LLM
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ aggregator.py         # Main aggregator class
â”‚   â”œâ”€â”€ browser.py            # Browser automation
â”‚   â””â”€â”€ evaluator.py          # Response evaluation logic
â”œâ”€â”€ llms/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chatGPT_automation.py                # ChatGPT LLM
â”‚   â”œâ”€â”€ deepSeek_automation.py               # DeepSeek LLM
â”‚   â”œâ”€â”€ factory.py                           # Factory for LLMs
â”‚   â”œâ”€â”€ grok_automation.py                   # Grok LLM
â”‚   â””â”€â”€ mistral_automation.py                # Mistral LLM
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ storage.py            # Result storage utilities
â”œâ”€â”€ README.md                 # Project description
â”œâ”€â”€ LICENSE.md                # License
â”œâ”€â”€ .gitignore                # Ignore files
â”œâ”€â”€ .dockerignore             # Ignore files
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ Dockerfile                # Dockerfile
â””â”€â”€ requirements.txt          # Dependencies
```

### Adding New LLMs

To add support for a new LLM:

1. Add configuration in [config/llm_configs.py](config/llm_configs.py):

```python
LLM_CONFIGS = {
    # Existing LLMs...
    "new_llm": {
        "url": "https://new-llm-website.com/",
        "input_selector": "css.selector.for.input",
        "response_selector": "css.selector.for.response",
        "wait_time": 30,  # seconds
    },
}
```

2. If the LLM requires special handling, modify [core/browser.py](core/browser.py) to accommodate its interface.

### How It Works

1. **Query Input**: The application takes a user query as input.
2. **Browser Automation**: It launches Chrome instances (headless by default) to interact with each LLM's web interface.
3. **Response Collection**: It submits the query to each LLM and waits for responses.
4. **Evaluation**: Responses are evaluated using:
   - Relevance to the original query (50% weight)
   - Cross-check similarity with other responses (30% weight)
   - Optimal length scoring (20% weight)
5. **Result Presentation**: The highest-scoring response is displayed and all responses are saved to a JSON file.

### Troubleshooting

- **Chrome Issues**: Make sure your Chrome browser and ChromeDriver versions are compatible.
- **Selector Problems**: If an LLM changes its web interface, update the CSS selectors in the configuration.
- **Timeout Errors**: Increase the `wait_time` in the LLM's configuration if responses are taking longer.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
